{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7156bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "# !pip install graphviz\n",
    "# !pip install seaborn\n",
    "# !pip install pydotplus\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix,accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz \n",
    "import seaborn as sns\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import norm\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas.api.types import CategoricalDtype \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee9c0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "df = pandas.read_csv('KDDTrain_Initial.csv')\n",
    "df_sub = pandas.read_csv('KDDTest_Initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab942458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor( BaseEstimator, TransformerMixin ):\n",
    "    def __init__( self, \n",
    "                 cols_tobe_dropped=[],\n",
    "                 log_vars = [],\n",
    "                 z_vars = [],\n",
    "                 minmax_vars = [],\n",
    "                 bin_vars = [],\n",
    "                 catunique_vars=[],\n",
    "                 categorical_vars=[]):\n",
    "        '''\n",
    "        Params\n",
    "        ---------------\n",
    "        outlier_vars: list\n",
    "            list of columns to be considered for removal when applying transform\n",
    "        '''\n",
    "        \n",
    "        # Variables to be dropped\n",
    "        self.cols_tobe_dropped = cols_tobe_dropped\n",
    "        \n",
    "        # Variables to scale down using log function\n",
    "        self.log_vars = [c for c in log_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply z normalization on\n",
    "        self.z_vars = [c for c in z_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply min max normalization on\n",
    "        self.minmax_vars = [c for c in minmax_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply binning on\n",
    "        self.bin_vars = [c for c in bin_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply categorization by unique value on\n",
    "        self.catunique_vars = [*catunique_vars, *bin_vars]\n",
    "        self.catunique_vars = [c for c in self.catunique_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to one hot encode\n",
    "        self.categorical_vars = [*categorical_vars, *catunique_vars]\n",
    "        self.categorical_vars = [c for c in self.categorical_vars if c not in self.cols_tobe_dropped]\n",
    "    \n",
    "\n",
    "    def fit( self, df, outlier_removal=False, target_col=None, target_value=None ):\n",
    "        self.target_col = target_col\n",
    "        self.target_value = target_value\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # drop columns\n",
    "        df_clean = df.drop(self.cols_tobe_dropped, axis=1)\n",
    "\n",
    "        # Apply Nan and non-finite values removal so that fitting would occur on clean data\n",
    "        df_clean = self.clean_nans(df_clean)\n",
    "        \n",
    "        \n",
    "        # Apply outlier removal so that fitting would occur on clean data\n",
    "        if outlier_removal:\n",
    "            df_clean = self.remove_outlier(df_clean)\n",
    "        \n",
    "        # Apply log2 transformation\n",
    "        self._min_log = [] # the minimum value for each log var\n",
    "        for v in self.log_vars:\n",
    "            self._min_log.append(min(df_clean[v]))\n",
    "            df_clean[v] = np.log2([v if v > 1 else 1 for v in (df_clean[v].values - min(df_clean[v]) + 1)])\n",
    "        \n",
    "        # Apply z normalization\n",
    "        self._trans_z = [] # the standard scaler transformers for each z var\n",
    "        for z in self.z_vars:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(np.reshape(df_clean[z].values, (-1,1)))\n",
    "            self._trans_z.append(scaler)\n",
    "\n",
    "        # Apply min max normalization\n",
    "        self._trans_minmax = [] # the standard scaler transformers for each z var\n",
    "        for mm in self.minmax_vars:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(np.reshape(df_clean[mm].values, (-1,1)))\n",
    "            self._trans_minmax.append(scaler)\n",
    "\n",
    "        # Apply binning ===============================================\n",
    "        self._bins = [] # the bins used for each bin var\n",
    "        for b in self.bin_vars:\n",
    "            _, bins = pd.qcut(df_clean[b].copy(), q=6, duplicates='drop', retbins=True)\n",
    "            # expand the bins boundaries to infinity\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            self._bins.append(bins)\n",
    "            \n",
    "            v = pd.cut(df_clean[b], bins, duplicates='drop')\n",
    "            df_clean[b] = v.astype(str)\n",
    "            \n",
    "        \n",
    "        # Apply categorization by unique value\n",
    "        self._cat_unique = [] # unique values for each cat unique vars\n",
    "        for cc in self.catunique_vars:\n",
    "            unique_values = np.unique(df_clean[cc])\n",
    "            freq_unique_values = []\n",
    "            \n",
    "            # Only take the values that are frequent (repeated more than 5% of the data)\n",
    "            for u in unique_values:\n",
    "                if sum(df_clean[cc] == u) > 0.05*len(df_clean):\n",
    "                    # we need to put the values here so that the categorization transformer would work\n",
    "                    df_clean.loc[df_clean[cc] == u, cc]  = str(u) \n",
    "                    freq_unique_values.append(u)\n",
    "        \n",
    "            # Set any value that was not in the transformation set to 'other'\n",
    "            \n",
    "            df_clean.loc[df_clean.applymap(np.isreal)[cc],cc] = \"other\" # same here\n",
    "            self._cat_unique.append(freq_unique_values)\n",
    "        \n",
    "        # Apply One-hot Encoding for categorical variables\n",
    "        cat_indices = [df_clean.columns.get_loc(col_name) for col_name in self.categorical_vars]\n",
    "        self._trans_cat = ColumnTransformer(transformers=[('cat', OneHotEncoder(), cat_indices)], remainder='drop', sparse_threshold=0)\n",
    "        self._trans_cat.fit(df_clean)\n",
    "        return self\n",
    "\n",
    "    def transform( self, df, outlier_removal=False ):\n",
    "        '''\n",
    "        Params\n",
    "        ---------------\n",
    "        outlier_removal: str -- Changed\n",
    "            strategy for outlier removal, default is None resulting in no outlier removal\n",
    "            possible string values:\n",
    "            \"before\" : apply outlier removal before preprocessing\n",
    "            \"after\" : apply outlier removal after preprocessing\n",
    "        '''\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Drop columns\n",
    "        df_clean = df.drop(self.cols_tobe_dropped, axis=1).reset_index().iloc[:,1:]\n",
    "        \n",
    "        # Apply Nan and non-finite values removal\n",
    "        df_clean = self.clean_nans(df_clean)\n",
    "        \n",
    "        # Apply outlier removal before preprocessing\n",
    "        if outlier_removal:\n",
    "            df_clean = self.remove_outlier(df_clean)\n",
    "\n",
    "        # Apply log2 transformation\n",
    "        for v, m in zip(self.log_vars, self._min_log):\n",
    "            df_clean[v] = np.log2([v if v > 1 else 1 for v in (df_clean[v].values - m + 1)])\n",
    "\n",
    "        # Apply z normalization\n",
    "        for z, trans in zip(self.z_vars, self._trans_z):\n",
    "            df_clean[z] = trans.transform(np.reshape(df_clean[z].values, (-1,1)))\n",
    "\n",
    "        # Apply min max normalization\n",
    "        for mm, trans in zip(self.minmax_vars, self._trans_minmax):\n",
    "            df_clean[mm] = trans.transform(np.reshape(df_clean[mm].values, (-1,1)))\n",
    "\n",
    "        # Apply binning ===============================================\n",
    "        for b, bins in zip(self.bin_vars, self._bins):\n",
    "            v = pd.cut(df_clean[b], bins, duplicates='drop')\n",
    "            df_clean[b] = v.astype(str)\n",
    "\n",
    "        # Apply categorization by unique value\n",
    "        for cc, unique_values in zip(self.catunique_vars, self._cat_unique):\n",
    "            # Set \"other' for unkown unique values \n",
    "            for u in unique_values:\n",
    "                df_clean.loc[df_clean[cc] == u, cc]  = str(u) \n",
    "            # Set any value that was not in the transformation set to 'other'\n",
    "            df_clean.loc[df_clean.applymap(np.isreal)[cc],cc] = \"other\"\n",
    "        \n",
    "            \n",
    "        # Apply One-hot Encoding for categorical variables\n",
    "        cat_indices = [df_clean.columns.get_loc(col_name) for col_name in self.categorical_vars]\n",
    "        \n",
    "        # Contains one hot encoded variables + dataframe variables\n",
    "        x = np.array(self._trans_cat.transform(df_clean))\n",
    "        \n",
    "        # get the columns names for the categorical data columns only\n",
    "        feature_names = [[f\"{c}_{u}\" for u in np.unique(df_clean[c])] for c in self.categorical_vars]\n",
    "        feature_names = [y for x in feature_names for y in x]\n",
    "\n",
    "        # drop categorical columns from the real dataframe\n",
    "        df_clean = df_clean.drop(self.categorical_vars, axis=1)\n",
    "\n",
    "        # create the categorical data frame (contains the categorical colummns)\n",
    "        df_cat = pd.DataFrame(x[:,:len(feature_names)], columns=feature_names).astype(float)\n",
    "        # concatenate the dataframe that contains no categorical columns and the one with categorical columns\n",
    "        df_clean = pd.concat([df_clean, df_cat], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def remove_outlier(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # remove numeric outliers\n",
    "        num_df = df.select_dtypes(include='number')\n",
    "        \n",
    "        if self.target_col:\n",
    "            df = df[~((np.abs(stats.zscore(num_df)) > 3).any(1) & np.array(df[self.target_col]==self.target_value))]\n",
    "        else:\n",
    "            df = df[~((np.abs(stats.zscore(num_df)) > 3).any(1))]\n",
    "\n",
    "#         print(len(df), sum(df[self.target_col]!=self.target_value))\n",
    "        # remove string (categorical) outliers that are less than 5% of the data\n",
    "        v = df.select_dtypes(include='object')\n",
    "        if self.target_col:\n",
    "            df = df[~(v.replace(v.stack().value_counts()).lt(0.05*len(df)).any(1)& np.array(df[self.target_col]==self.target_value))]\n",
    "        else:\n",
    "            df = df[~(v.replace(v.stack().value_counts()).lt(0.05*len(df)).any(1))]\n",
    "#         print(len(df),sum(df[self.target_col]!=self.target_value))\n",
    "\n",
    "        return df.reset_index().iloc[:,1:]\n",
    "    \n",
    "    def clean_nans(self, df):\n",
    "        # Remove NaNs\n",
    "        df.dropna(inplace=True)\n",
    "        df = df.reset_index().iloc[:,1:]\n",
    "        \n",
    "        # Remove non finit values\n",
    "        num_df = df.select_dtypes(include='number')\n",
    "        indices = np.unique(np.array(np.where(~(np.isfinite(num_df))))[0])\n",
    "        df.drop(indices, inplace=True)\n",
    "        df = df.reset_index().iloc[:,1:]\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e932476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMEDS~1\\AppData\\Local\\Temp/ipykernel_4664/3060070776.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean_sub[\"service\"][indices[0][0]] = df_clean_sub[\"service\"][0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['ID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AHMEDS~1\\AppData\\Local\\Temp/ipykernel_4664/3060070776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mskb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutlier_removal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mdf_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutlier_removal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AHMEDS~1\\AppData\\Local\\Temp/ipykernel_4664/2416798888.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, df, outlier_removal, target_col, target_value)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# drop columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mdf_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols_tobe_dropped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Apply Nan and non-finite values removal so that fitting would occur on clean data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['ID'] not found in axis\""
     ]
    }
   ],
   "source": [
    "cols_tobe_dropped = [\n",
    "                    'urgent',\n",
    "                    'land',\n",
    "                    'duration',\n",
    "                    'ID',\n",
    "                    'hot',\n",
    "                    'num_failed_logins',\n",
    "                    'wrong_fragment',\n",
    "                    'num_file_creations',\n",
    "                    'root_shell',\n",
    "                    \"num_compromised\",\n",
    "                    'su_attempted',\n",
    "                    'num_root',\n",
    "                    'num_shells',\n",
    "                    'num_outbound_cmds',\n",
    "                    'is_host_login',\n",
    "                    'is_guest_login',\n",
    "                    'flag',\n",
    "                    'service',\n",
    "#                     'protocol_type',\n",
    "                    'dst_host_serror_rate',\n",
    "                    'dst_host_srv_serror_rate',\n",
    "                    'dst_host_rerror_rate',\n",
    "                    'dst_host_srv_rerror_rate',\n",
    "                    'num_access_files',\n",
    "#                     'src_bytes',\n",
    "#                     'dst_bytes', \n",
    "                    'srv_serror_rate',\n",
    "                    'srv_rerror_rate',\n",
    "                    'serror_rate',\n",
    "                    'srv_diff_host_rate',\n",
    "#                     'dst_host_same_srv_rate',\n",
    "#                     'logged_in',\n",
    "                    'rerror_rate',\n",
    "                    'dst_host_srv_diff_host_rate',\n",
    "#                     'level',\n",
    "#                     'count',\n",
    "                    ]\n",
    "\n",
    "# Variables to scale down using log function\n",
    "log_vars = ['src_bytes', 'dst_bytes', 'count', \"srv_count\"]\n",
    "log_vars = [c for c in log_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply z normalization on\n",
    "z_vars = ['src_bytes', 'dst_bytes', 'count', 'srv_count', 'level', \"wrong_fragment\"]\n",
    "z_vars = [c for c in z_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply min max normalization on\n",
    "minmax_vars = ['dst_host_count', 'dst_host_srv_count']\n",
    "minmax_vars = [c for c in minmax_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply binning on\n",
    "bin_vars = ['same_srv_rate', 'diff_srv_rate']\n",
    "bin_vars = [c for c in bin_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply categorization by unique value on\n",
    "catunique_vars = ['dst_host_srv_diff_host_rate', *bin_vars]\n",
    "catunique_vars = [c for c in catunique_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to one hot encode\n",
    "categorical_vars = [\"protocol_type\", \"service\", \"flag\", *catunique_vars]\n",
    "categorical_vars = [c for c in categorical_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Replace this one value \"tftp_u\" in the test data\n",
    "df_clean_sub = df_sub.copy()\n",
    "indices = np.where(df_clean_sub[\"service\"] == \"tftp_u\")\n",
    "df_clean_sub[\"service\"][indices[0][0]] = df_clean_sub[\"service\"][0] \n",
    "\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    cols_tobe_dropped=cols_tobe_dropped,\n",
    "    log_vars = log_vars,\n",
    "    z_vars = z_vars,\n",
    "    minmax_vars = minmax_vars,\n",
    "    bin_vars = bin_vars,\n",
    "    catunique_vars=catunique_vars,\n",
    "    categorical_vars=categorical_vars)\n",
    "skb = SelectKBest(f_classif, k=20)\n",
    "\n",
    "preprocessor.fit(df, outlier_removal=True, target_col='Class', target_value=0)\n",
    "\n",
    "df_pre = preprocessor.transform(df, outlier_removal=True)\n",
    "df_sub_pre = preprocessor.transform(df_clean_sub)\n",
    "\n",
    "x_train = df_pre.loc[:, df_pre.columns != \"Class\"]\n",
    "df_sub_pre  = df_sub_pre.loc[:, df_sub_pre.columns != \"Class\"]\n",
    "y_train = df_pre[\"Class\"]\n",
    "\n",
    "skb.fit(x_train,y_train)\n",
    "x_train = pd.DataFrame(skb.transform(x_train), columns=skb.get_feature_names_out())\n",
    "df_sub_pre = pd.DataFrame(skb.transform(df_sub_pre), columns=skb.get_feature_names_out())\n",
    "\n",
    "# reduce the features to 2D\n",
    "# pca = PCA(n_components=16)\n",
    "# x_reduced     = pca.fit_transform(x_enc)\n",
    "# x_reduced_sub = pca.transform(x_enc_sub)\n",
    "# x_reduced     = x_enc\n",
    "# x_reduced_sub = x_enc_sub\n",
    "\n",
    "# print(pca.explained_variance_)\n",
    "\n",
    "\n",
    "# vis_indices = np.arange(0,1000)\n",
    "y = np.array(y_train)#[vis_indices]\n",
    "x = np.array(x_train)#[vis_indices]\n",
    "x_sub = np.array(df_sub_pre)#[vis_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9668b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeCateg(df, testing):\n",
    "    for i in range(0,len(categ)):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[categ[i]])\n",
    "        df[categ[i]] = le.transform(df[categ[i]])\n",
    "        testing[categ[i]] = le.transform(testing[categ[i]])\n",
    "    return df, testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99709e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = ['protocol_type','service','flag','attack']\n",
    "\n",
    "df , testing=ChangeCateg(df, testing)\n",
    "# Encode Categorical Columns\n",
    "#le = LabelEncoder()\n",
    "#le.fit(df[categ[0]])\n",
    "#df[categ] = df[categ].apply(le.fit_transform)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50c7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "22544\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70acfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'protocol_type',\n",
       " 'service',\n",
       " 'flag',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'land',\n",
       " 'wrong_fragment',\n",
       " 'urgent',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'su_attempted',\n",
       " 'num_root',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'num_access_files',\n",
       " 'num_outbound_cmds',\n",
       " 'is_host_login',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'attack']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50adcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(df):\n",
    "    drop=['num_outbound_cmds',]\n",
    "    df = df.drop(drop,axis=1)\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    X = df\n",
    "    y= X.attack\n",
    "    X = X.drop(['attack'],axis=1)\n",
    "    \n",
    "    categ=['protocol_type','service','flag']\n",
    "    X_categ= X[categ]\n",
    "    X = X.drop(categ,axis=1)\n",
    "    #Scaling\n",
    "    categories=X.columns.values.tolist()\n",
    "    transformer_MinMax = MinMaxScaler().fit(X_categ)\n",
    "    X_categ = transformer_MinMax.fit_transform(X_categ)\n",
    "    \n",
    "    transformer_Robust = RobustScaler().fit(X)\n",
    "    #X = transformer_Robust.transform(X)\n",
    "    \n",
    "    transformer_Standard = StandardScaler().fit(X)\n",
    "    X = transformer_Standard.transform(X)\n",
    "    \n",
    "    #concat\n",
    "    print(type(X_categ))\n",
    "    X_categ=pd.DataFrame(X_categ, columns = categ)\n",
    "    X=pd.DataFrame(X, columns = categories)\n",
    "    X = pd.concat([X, X_categ], axis=1)\n",
    "    \n",
    "    return X, y, transformer_MinMax, transformer_Robust, transformer_Standard\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d34529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestPreprocessing(df, transformer_MinMax, transformer_Robust, transformer_Standard):\n",
    "    drop=['num_outbound_cmds']\n",
    "    df = df.drop(drop,axis=1)\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    X = df\n",
    "    y= X.attack\n",
    "    X = X.drop(['attack'],axis=1)\n",
    "    \n",
    "    categ=['protocol_type','service','flag']\n",
    "    X_categ= X[categ]\n",
    "    X = X.drop(categ,axis=1)\n",
    "    #Scaling\n",
    "    categories=X.columns.values.tolist()\n",
    "    #transformer_MinMax = MinMaxScaler().fit(X_categ)\n",
    "    X_categ = transformer_MinMax.fit_transform(X_categ)\n",
    "    \n",
    "    #transformer_Robust = RobustScaler().fit(X)\n",
    "    #X = transformer_Robust.transform(X)\n",
    "    \n",
    "    #transformer_Standard = StandardScaler().fit(X)\n",
    "    X = transformer_Standard.transform(X)\n",
    "    \n",
    "    #concat\n",
    "    print(type(X_categ))\n",
    "    X_categ=pd.DataFrame(X_categ, columns = categ)\n",
    "    X=pd.DataFrame(X, columns = categories)\n",
    "    X = pd.concat([X, X_categ], axis=1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df15bd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X, y, transformer_MinMax, transformer_Robust, transformer_Standard= Preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8455059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test =TestPreprocessing(testing, transformer_MinMax, transformer_Robust, transformer_Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01fce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7a1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cdce5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100764\n",
      "100764\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.assign(e=pd.Series(y_train).values)\n",
    "X_train.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_train))\n",
    "X_train.drop_duplicates()\n",
    "print(len(X_train))\n",
    "X_train.to_csv('./Train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3059b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25192\n",
      "25192\n"
     ]
    }
   ],
   "source": [
    "X_val = X_val.assign(e=pd.Series(y_val).values)\n",
    "X_val.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_val))\n",
    "X_val.drop_duplicates()\n",
    "print(len(X_val))\n",
    "X_val.to_csv('./Val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e659b396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22538\n",
      "22538\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.assign(e=pd.Series(y_test).values)\n",
    "X_test.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_test))\n",
    "X_test.drop_duplicates()\n",
    "print(len(X_test))\n",
    "X_test.to_csv('./Test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
