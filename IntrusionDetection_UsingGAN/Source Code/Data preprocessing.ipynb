{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7156bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T01:28:05.797856Z",
     "start_time": "2021-12-11T01:28:05.787851Z"
    }
   },
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "# !pip install graphviz\n",
    "# !pip install seaborn\n",
    "# !pip install pydotplus\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.feature_selection import f_classif, SelectPercentile\n",
    "\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix,accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz \n",
    "import seaborn as sns\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import norm\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "# import xgboost as xgb\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "# from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas.api.types import CategoricalDtype \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee9c0d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T01:59:42.527483Z",
     "start_time": "2021-12-11T01:59:42.170266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\anaconda3\\envs\\main\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Mohamed\\anaconda3\\envs\\main\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(50)\n",
    "df = pandas.read_csv('KDDTrain_Initial.csv')\n",
    "df_sub = pandas.read_csv('KDDTest_Initial.csv')\n",
    "df[\"attack\"][df[\"attack\"] == \"normal\"] = 0;     df[\"attack\"][df[\"attack\"] == \"attack\"] = 1\n",
    "df_sub[\"attack\"][df_sub[\"attack\"] == \"normal\"] = 0; df_sub[\"attack\"][df_sub[\"attack\"] == \"attack\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab942458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T16:34:21.726112Z",
     "start_time": "2021-12-11T16:34:21.726112Z"
    },
    "code_folding": [
     105,
     121,
     173,
     286
    ]
   },
   "outputs": [],
   "source": [
    "class Preprocessor( BaseEstimator, TransformerMixin ):\n",
    "    def __init__( self, \n",
    "                 cols_tobe_dropped=[],\n",
    "                 log_vars = [],\n",
    "                 z_vars = [],\n",
    "                 minmax_vars = [],\n",
    "                 bin_vars = [],\n",
    "                 catunique_vars=[],\n",
    "                 labelencode_vars=[],\n",
    "                 onehot_vars=[],\n",
    "                 lowvariance_thr=1,\n",
    "                 lowvariance_ignore=[],\n",
    "                 percentile=10\n",
    "                ):\n",
    "        '''\n",
    "        Params\n",
    "        ---------------\n",
    "        outlier_vars: list\n",
    "            list of columns to be considered for removal when applying transform\n",
    "        '''\n",
    "        \n",
    "        # Variables to be dropped\n",
    "        self.cols_tobe_dropped = cols_tobe_dropped\n",
    "        \n",
    "        # Variables to scale down using log function\n",
    "        self.log_vars = [c for c in log_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply z normalization on\n",
    "        self.z_vars = [c for c in z_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply min max normalization on\n",
    "        self.minmax_vars = [c for c in minmax_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        self.percentile = percentile\n",
    "        \n",
    "        # Variables to apply binning on\n",
    "        self.bin_vars = [c for c in bin_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to apply categorization by unique value on\n",
    "        self.catunique_vars = [*catunique_vars, *bin_vars]\n",
    "        self.catunique_vars = [c for c in self.catunique_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Variables to one hot encode\n",
    "        self.onehot_vars = [*onehot_vars]\n",
    "        self.onehot_vars = [c for c in self.onehot_vars if c not in self.cols_tobe_dropped]\n",
    "        \n",
    "        # Variables to one label encode\n",
    "        self.labelencode_vars = [c for c in labelencode_vars if c not in self.cols_tobe_dropped]\n",
    "\n",
    "        # Vaiables that must be treated as categorical from the time it entered\n",
    "        self.object_vars = self.labelencode_vars + self.onehot_vars\n",
    "        self.object_vars = [v for v in self.object_vars if v not in \\\n",
    "                            self.catunique_vars + self.log_vars + self.z_vars + self.minmax_vars]\n",
    "        \n",
    "        self.lowvariance_thr = lowvariance_thr\n",
    "        \n",
    "        self.lowvariance_ignore = lowvariance_ignore\n",
    "        \n",
    "    def fit( self, df, outlier_removal=False, target_col=None, target_value=None ):\n",
    "        self.target_col = target_col\n",
    "        self.target_value = target_value\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # drop columns\n",
    "        df_clean = df.drop(self.cols_tobe_dropped, axis=1)\n",
    "\n",
    "        # Apply Nan and non-finite values removal so that fitting would occur on clean data\n",
    "        df_clean = self.clean_nans(df_clean)\n",
    "        \n",
    "        # Change categorical variable columns to object so that categorical outlier removal may apply\n",
    "        for col in self.object_vars:\n",
    "            df_clean[col] = df_clean[col].astype(object)\n",
    "        for col in [c for c in df_clean.columns if c not in self.object_vars]:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col])\n",
    "                \n",
    "        # Apply outlier removal so that fitting would occur on clean data\n",
    "        # perhaps outlier removal should follow log transformation\n",
    "        if outlier_removal:\n",
    "            df_clean = self.remove_outlier(df_clean)\n",
    "        \n",
    "        # Apply log2 transformation\n",
    "        self._min_log = {} # the minimum value for each log var\n",
    "        for v in self.log_vars:\n",
    "            self._min_log[v] = min(df_clean[v])\n",
    "            df_clean[v] = np.log2([v if v > 1 else 1 for v in (df_clean[v].values - min(df_clean[v]) + 1)])\n",
    "        \n",
    "        # Apply z normalization\n",
    "        self._trans_z = {} # the standard scaler transformers for each z var\n",
    "        for z in self.z_vars:\n",
    "            scaler = StandardScaler()\n",
    "            df_clean[z] = scaler.fit_transform(np.reshape(df_clean[z].values, (-1,1)))\n",
    "            self._trans_z[z]=scaler\n",
    "\n",
    "        # Apply min max normalization\n",
    "        self._trans_minmax = {} # the standard scaler transformers for each z var\n",
    "        for mm in self.minmax_vars:\n",
    "            scaler = MinMaxScaler()\n",
    "            df_clean[mm] = scaler.fit_transform(np.reshape(df_clean[mm].values, (-1,1)))\n",
    "            self._trans_minmax[mm]=scaler\n",
    "\n",
    "        # Apply feature selection\n",
    "        str_df = df_clean.select_dtypes(include='object')\n",
    "        num_df = df_clean.select_dtypes(include='number')\n",
    "        self.selected_columns = df_clean.columns\n",
    "        if self.target_col and self.percentile:\n",
    "            self.sel = SelectPercentile(f_classif, percentile=self.percentile).\\\n",
    "            fit( num_df.loc[:, num_df.columns != self.target_col], df_clean[self.target_col])\n",
    "            self.selected_columns = [*self.sel.get_feature_names_out(), *str_df.columns]\n",
    "            if self.target_col not in self.selected_columns: self.selected_columns.append(self.target_col)\n",
    "            df_clean = df_clean[self.selected_columns]\n",
    "            \n",
    "            # Readjust cat unieuq, label encode and one-hot encoding vars\n",
    "            self.labelencode_vars = [c for c in self.labelencode_vars if c in self.selected_columns]\n",
    "            self.onehot_vars = [c for c in self.onehot_vars if c in self.selected_columns]\n",
    "            self.catunique_vars = [c for c in self.catunique_vars if c in self.selected_columns]\n",
    "            self.bin_vars = [c for c in self.bin_vars if c in self.selected_columns]\n",
    "\n",
    "            \n",
    "        # Apply binning ===============================================\n",
    "        self._bins = {} # the bins used for each bin var\n",
    "        for b in self.bin_vars:\n",
    "            _, bins = pd.qcut(df_clean[b].copy(), q=6, duplicates='drop', retbins=True)\n",
    "            # expand the bins boundaries to infinity\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            self._bins[b]=bins\n",
    "            \n",
    "            v = pd.cut(df_clean[b], bins, duplicates='drop')\n",
    "            df_clean[b] = v.astype(str)\n",
    "            \n",
    "        # Apply categorization by unique value\n",
    "        self._cat_unique = {} # unique values for each cat unique vars\n",
    "        for cc in self.catunique_vars:\n",
    "            unique_values = np.unique(df_clean[cc])\n",
    "            freq_unique_values = []\n",
    "            \n",
    "            # Only take the values that are frequent (repeated more than 5% of the data)\n",
    "            for u in unique_values:\n",
    "                if sum(df_clean[cc] == u) > 0.05*len(df_clean):\n",
    "                    # we need to put the values here so that the categorization transformer would work\n",
    "                    df_clean.loc[df_clean[cc] == u, cc]  = str(u) \n",
    "                    freq_unique_values.append(u)\n",
    "        \n",
    "            # Set any value that was not in the transformation set to 'other'\n",
    "            df_clean.loc[df_clean.applymap(np.isreal)[cc],cc] = \"other\" # same here\n",
    "            self._cat_unique[cc]=freq_unique_values\n",
    "        \n",
    "        # Apply low variance removal on columns with dominant values\n",
    "#         df_var = df_clean.copy()\n",
    "#         df_var = df_var.loc[:, [c for c in df_clean.columns if c not in self.lowvariance_ignore]]\n",
    "#         string_features = df_var.select_dtypes(include='object')\n",
    "#         for str_fea in string_features.columns:\n",
    "#             df_var[str_fea] = LabelEncoder().fit_transform(df_var[str_fea])\n",
    "#         self.sel = VarianceThreshold(threshold=(self.lowvariance_thr))\n",
    "#         self.sel = self.sel.fit(df_var)\n",
    "        \n",
    "#         df_clean = df_clean[[*self.sel.get_feature_names_out(), *self.lowvariance_ignore]]\n",
    "        \n",
    "        # Apply label Encoding for specified categorical variables\n",
    "        self._le_cat = {} # label encoders for each specified column\n",
    "        for le_var in self.labelencode_vars:\n",
    "            le = LabelEncoder().fit(df_clean[le_var])\n",
    "            self._le_cat[le_var]=le\n",
    "        \n",
    "        # Apply One-hot Encoding for categorical variables\n",
    "#         self._trans_cat = None\n",
    "#         if len(self.onehot_vars) > 0:\n",
    "        cat_indices = [df_clean.columns.get_loc(col_name) for col_name in self.onehot_vars]\n",
    "        self._trans_cat = ColumnTransformer(transformers=[('cat', OneHotEncoder(), cat_indices)], remainder='drop', sparse_threshold=0)\n",
    "        self._trans_cat.fit(df_clean) \n",
    "        return self\n",
    "\n",
    "    def transform( self, df, outlier_removal=False ):\n",
    "        '''\n",
    "        Params\n",
    "        ---------------\n",
    "        outlier_removal: str -- Changed\n",
    "            strategy for outlier removal, default is None resulting in no outlier removal\n",
    "            possible string values:\n",
    "            \"before\" : apply outlier removal before preprocessing\n",
    "            \"after\" : apply outlier removal after preprocessing\n",
    "        '''\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Drop columns\n",
    "        df_clean = df.drop(self.cols_tobe_dropped, axis=1).reset_index().iloc[:,1:]\n",
    "        \n",
    "        # Apply Nan and non-finite values removal\n",
    "        df_clean = self.clean_nans(df_clean)\n",
    "\n",
    "        # Change categorical variable columns to object so that categorical outlier removal may apply\n",
    "        for col in self.labelencode_vars + self.onehot_vars:\n",
    "            df_clean[col] = df_clean[col].astype(object)\n",
    "        \n",
    "        # Apply log2 transformation\n",
    "        for v, m in self._min_log.items():\n",
    "            df_clean[v] = np.log2([v if v > 1 else 1 for v in (df_clean[v].values - m + 1)])\n",
    "\n",
    "        # Apply z normalization\n",
    "        for z, trans in self._trans_z.items():\n",
    "            df_clean[z] = trans.transform(np.reshape(df_clean[z].values, (-1,1)))\n",
    "\n",
    "        # Apply min max normalization\n",
    "        for mm, trans in self._trans_minmax.items():\n",
    "            df_clean[mm] = trans.transform(np.reshape(df_clean[mm].values, (-1,1)))\n",
    "\n",
    "        # Apply low variance removal on columns with dominant values\n",
    "        df_clean = df_clean[self.selected_columns]\n",
    "            \n",
    "        # Apply outlier removal before preprocessing\n",
    "        if outlier_removal:\n",
    "            df_clean = self.remove_outlier(df_clean)\n",
    "            \n",
    "        # Apply binning ===============================================\n",
    "        for b, bins in self._bins.items():\n",
    "            v = pd.cut(df_clean[b], bins, duplicates='drop')\n",
    "            df_clean[b] = v.astype(str)\n",
    "\n",
    "        # Apply categorization by unique value\n",
    "        for cc, unique_values in self._cat_unique.items():\n",
    "            # Set \"other' for unkown unique values \n",
    "            for u in unique_values:\n",
    "                df_clean.loc[df_clean[cc] == u, cc]  = str(u) \n",
    "            # Set any value that was not in the transformation set to 'other'\n",
    "            df_clean.loc[df_clean.applymap(np.isreal)[cc],cc] = \"other\"\n",
    "        \n",
    "        # Contains one hot encoded variables + dataframe variables\n",
    "        for le_var, le in self._le_cat.items():\n",
    "            df_clean[le_var] = le.transform(df_clean[le_var])\n",
    "        \n",
    "        # Apply One-hot Encoding for categorical variables\n",
    "        cat_indices = [df_clean.columns.get_loc(col_name) for col_name in self.onehot_vars]\n",
    "        \n",
    "        # Contains one hot encoded variables + dataframe variables\n",
    "        x = np.array(self._trans_cat.transform(df_clean))\n",
    "        \n",
    "        # get the columns names for the categorical data columns only\n",
    "        feature_names = [[f\"{c}_{u}\" for u in np.unique(df_clean[c])] for c in self.onehot_vars]\n",
    "        feature_names = [y for x in feature_names for y in x]\n",
    "\n",
    "        # drop categorical columns from the real dataframe\n",
    "        df_clean = df_clean.drop(self.onehot_vars, axis=1)\n",
    "\n",
    "        # create the categorical data frame (contains the categorical colummns)\n",
    "        df_cat = pd.DataFrame(x[:,:len(feature_names)], columns=feature_names).astype(float)\n",
    "        # concatenate the dataframe that contains no categorical columns and the one with categorical columns\n",
    "        df_clean = pd.concat([df_clean, df_cat], axis=1)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, df, outlier_removal=False, target_col=None, target_value=None ):\n",
    "        self.fit(df, outlier_removal=outlier_removal, target_col=target_col, target_value=target_value )\n",
    "        return self.transform(df, outlier_removal=outlier_removal)\n",
    "    \n",
    "    \n",
    "    def remove_outlier(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # remove numeric outliers\n",
    "        num_df = df.select_dtypes(include='number')\n",
    "        \n",
    "        \n",
    "#         print(len(df))\n",
    "        if self.target_col and self.target_value is not None:\n",
    "            df = df[~((np.abs(stats.zscore(num_df)) > 3).any(1) & np.array(df[self.target_col]==self.target_value))]\n",
    "        else:\n",
    "            df = df[~((np.abs(stats.zscore(num_df)) > 3).any(1))]\n",
    "            \n",
    "#         print(len(df))\n",
    "#         for col in ['flag']:\n",
    "#             print(\"\\n\"+col)\n",
    "#             print(np.unique(df[col], return_counts=True))\n",
    "        # remove string (categorical) outliers that are less than 5% of the data\n",
    "        v = df.select_dtypes(include='object')\n",
    "        if self.target_col is not None and self.target_value is not None:\n",
    "            df = df[~(v.replace(v.stack().value_counts()).lt(0.05*len(df)).any(1)& np.array(df[self.target_col]==self.target_value))]\n",
    "        else:\n",
    "            df = df[~(v.replace(v.stack().value_counts()).lt(0.05*len(df)).any(1))]\n",
    "#         print(len(df))\n",
    "#         for col in ['flag']:\n",
    "#             print(\"\\n\"+col)\n",
    "#             print(np.unique(df[col], return_counts=True))\n",
    "        return df.reset_index().iloc[:,1:]\n",
    "    \n",
    "    def clean_nans(self, df):\n",
    "        # Remove NaNs\n",
    "        df.dropna(inplace=True)\n",
    "        df = df.reset_index().iloc[:,1:]\n",
    "        \n",
    "        # Remove non finit values\n",
    "        num_df = df.select_dtypes(include='number')\n",
    "        indices = np.unique(np.array(np.where(~(np.isfinite(num_df))))[0])\n",
    "        df.drop(indices, inplace=True)\n",
    "        df = df.reset_index().iloc[:,1:]\n",
    "        \n",
    "        return df\n",
    "\n",
    "# df_pre = preprocessor.transform(df.iloc[:10000,:], outlier_removal=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e932476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T16:37:37.233402Z",
     "start_time": "2021-12-11T16:36:57.621215Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "cols_tobe_dropped = [\n",
    "                    'urgent',\n",
    "                    'land',\n",
    "                    'duration',\n",
    "                    'hot',\n",
    "                    'num_failed_logins',\n",
    "                    'wrong_fragment',\n",
    "                    'num_file_creations',\n",
    "                    'root_shell',\n",
    "                    \"num_compromised\",\n",
    "                    'su_attempted',\n",
    "                    'num_root',\n",
    "                    'num_shells',\n",
    "                    'num_outbound_cmds',\n",
    "                    'is_host_login',\n",
    "                    'is_guest_login',\n",
    "#                     'flag',\n",
    "#                     'service',\n",
    "#                     'protocol_type',\n",
    "                    'dst_host_serror_rate',\n",
    "                    'dst_host_srv_serror_rate',\n",
    "                    'dst_host_rerror_rate',\n",
    "                    'dst_host_srv_rerror_rate',\n",
    "                    'num_access_files',\n",
    "#                     'src_bytes',\n",
    "#                     'dst_bytes', \n",
    "                    'srv_serror_rate',\n",
    "                    'srv_rerror_rate',\n",
    "                    'serror_rate',\n",
    "                    'srv_diff_host_rate',\n",
    "#                     'dst_host_same_srv_rate',\n",
    "#                     'logged_in',\n",
    "                    'rerror_rate',\n",
    "                    'dst_host_srv_diff_host_rate',\n",
    "#                     'level',\n",
    "#                     'count',\n",
    "                    ]\n",
    "\n",
    "# Variables to scale down using log function\n",
    "log_vars = ['src_bytes', 'dst_bytes', 'count', \"srv_count\"]\n",
    "log_vars = [c for c in log_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply z normalization on\n",
    "z_vars = ['src_bytes', 'dst_bytes', 'count', 'srv_count', \"wrong_fragment\"]\n",
    "z_vars = [c for c in z_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply min max normalization on\n",
    "minmax_vars = ['dst_host_count', 'dst_host_srv_count']\n",
    "minmax_vars = [c for c in minmax_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply binning on\n",
    "bin_vars = ['same_srv_rate', 'diff_srv_rate']\n",
    "bin_vars = [c for c in bin_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to apply categorization by unique value on\n",
    "catunique_vars = ['dst_host_srv_diff_host_rate', *bin_vars]\n",
    "catunique_vars = [c for c in catunique_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Variables to one hot encode\n",
    "labelencode_vars = [\"protocol_type\", \"service\", \"flag\", *catunique_vars]\n",
    "labelencode_vars = [c for c in labelencode_vars if c not in cols_tobe_dropped]\n",
    "\n",
    "# Replace this one value \"tftp_u\" in the test data\n",
    "df_clean_sub = df_sub.copy()\n",
    "indices = np.where(df_clean_sub[\"service\"] == \"ntp_u\")\n",
    "df_clean_sub[\"service\"][indices[0]] = df_clean_sub[\"service\"][0] \n",
    "indices = np.where(df_clean_sub[\"service\"] == \"tftp_u\")\n",
    "df_clean_sub[\"service\"][indices[0]] = df_clean_sub[\"service\"][0] \n",
    "\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    cols_tobe_dropped=cols_tobe_dropped,\n",
    "    log_vars = log_vars,\n",
    "    z_vars = z_vars,\n",
    "    minmax_vars = minmax_vars,\n",
    "    bin_vars = bin_vars,\n",
    "    catunique_vars=catunique_vars,\n",
    "    labelencode_vars=labelencode_vars,\n",
    "    onehot_vars=[],\n",
    "    percentile=0)\n",
    "\n",
    "preprocessor.fit(df.iloc[:,:], outlier_removal=1, target_col='attack', target_value=0)\n",
    "\n",
    "df_pre = preprocessor.transform(df.iloc[:,:], outlier_removal=1)\n",
    "df_sub_pre = preprocessor.transform(df_clean_sub)\n",
    "\n",
    "X = df_pre.loc[:, df_pre.columns != \"attack\"]\n",
    "y = df_pre[\"attack\"]\n",
    "X_sub  = df_sub_pre.loc[:, df_sub_pre.columns != \"attack\"]\n",
    "y_sub = df_sub_pre[\"attack\"]\n",
    "\n",
    "# X = np.array(x_train)\n",
    "# y = np.array(y_train)\n",
    "# X_sub = np.array(X_sub)\n",
    "# y_sub = np.array(y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b81bf59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T16:37:53.894381Z",
     "start_time": "2021-12-11T16:37:52.381088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92139\n",
      "92139\n",
      "23035\n",
      "23035\n",
      "22544\n",
      "22544\n"
     ]
    }
   ],
   "source": [
    "# Split and save tthe data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "\n",
    "X_train = X_train.assign(e=pd.Series(y_train).values)\n",
    "X_train.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_train))\n",
    "X_train.drop_duplicates()\n",
    "print(len(X_train))\n",
    "X_train.to_csv('./Train.csv', index=False)\n",
    "\n",
    "X_val = X_val.assign(e=pd.Series(y_val).values)\n",
    "X_val.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_val))\n",
    "X_val.drop_duplicates()\n",
    "print(len(X_val))\n",
    "X_val.to_csv('./NoramTest.csv', index=False)\n",
    "\n",
    "X_sub = X_sub.assign(e=pd.Series(y_sub).values)\n",
    "X_sub.rename({'e': 'attack'}, axis=1, inplace=True)\n",
    "print(len(X_sub))\n",
    "X_sub.drop_duplicates()\n",
    "print(len(X_sub))\n",
    "X_sub.to_csv('./SpecificTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c133a95d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T16:45:43.216659Z",
     "start_time": "2021-12-11T16:45:43.089754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "protocol_type\n",
      "(array([0, 1, 2]), array([ 6982, 97564, 10628], dtype=int64))\n",
      "\n",
      "service\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]), array([    1,     6,   862,     2,   719,   710,   734,   545,   563,\n",
      "         521,   538,   531,  7441,   434,  4089,  2887,   485,   474,\n",
      "        1222,   836,  6198,   518,     2,   460, 39939,     1,   530,\n",
      "           2,   644,   687,   433,   299,   410,   475,   429,   439,\n",
      "         451,   405,   347,   362,   360,   630,   296,  1755,     5,\n",
      "          78,    78,    69, 21509,    78,    86,    61,  6786,   245,\n",
      "         306,   381,   544,   477,  1436,     3,   578,     3,   780,\n",
      "         689,   617,   693], dtype=int64))\n",
      "\n",
      "flag\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([   35, 11152,  1343,   103,  2275, 34821,     4,     8,     4,\n",
      "       65160,   269], dtype=int64))\n",
      "\n",
      "src_bytes\n",
      "(array([-1.02865447, -0.79838091, -0.49397583, ...,  5.73426394,\n",
      "        5.90736923,  5.9629075 ]), array([49002,  2424,     2, ...,     1,     1,     1], dtype=int64))\n",
      "\n",
      "dst_bytes\n",
      "(array([-0.84283256, -0.64789952, -0.45296649, ...,  3.5037204 ,\n",
      "        4.72766632,  5.06107522]), array([64843,    15,     1, ...,     1,     2,     1], dtype=int64))\n",
      "\n",
      "logged_in\n",
      "(array([0, 1], dtype=int64), array([69113, 46061], dtype=int64))\n",
      "\n",
      "count\n",
      "(array([-1.65347157, -1.28850917, -1.07501985, -0.92354677, -0.80605512,\n",
      "       -0.71005745, -0.62889258, -0.55858437, -0.49656813, -0.44109272,\n",
      "       -0.3909091 , -0.34509505, -0.30295021, -0.26393018, -0.2276034 ,\n",
      "       -0.19362197, -0.16170132, -0.13160573, -0.10313775, -0.07613032,\n",
      "       -0.05044086, -0.0259467 , -0.00254154,  0.01986735,  0.04136133,\n",
      "        0.06201219,  0.08188358,  0.10103222,  0.11950883,  0.137359  ,\n",
      "        0.15462381,  0.17134043,  0.18754261,  0.20326108,  0.21852387,\n",
      "        0.23335667,  0.24778303,  0.26182465,  0.27550151,  0.28883208,\n",
      "        0.30183347,  0.31452154,  0.32691104,  0.3390157 ,  0.35084832,\n",
      "        0.36242086,  0.37374451,  0.38482975,  0.39568641,  0.40632373,\n",
      "        0.4167504 ,  0.42697459,  0.43700403,  0.44684598,  0.45650735,\n",
      "        0.46599462,  0.47531397,  0.48447123,  0.49347196,  0.5023214 ,\n",
      "        0.51102456,  0.51958621,  0.52801086,  0.53630283,  0.54446624,\n",
      "        0.55250501,  0.5604229 ,  0.56822348,  0.57591018,  0.58348627,\n",
      "        0.5909549 ,  0.59831907,  0.60558166,  0.61274543,  0.61981305,\n",
      "        0.62678705,  0.63366989,  0.64046391,  0.64717138,  0.65379448,\n",
      "        0.6603353 ,  0.66679587,  0.67317812,  0.67948394,  0.68571513,\n",
      "        0.69187344,  0.69796055,  0.7039781 ,  0.70992764,  0.71581072,\n",
      "        0.72162878,  0.72738326,  0.73307552,  0.73870691,  0.7442787 ,\n",
      "        0.74979215,  0.75524846,  0.76064881,  0.76599433,  0.77128613,\n",
      "        0.77652527,  0.7817128 ,  0.78684971,  0.79193699,  0.79697559,\n",
      "        0.80196643,  0.8069104 ,  0.81180838,  0.81666122,  0.82146975,\n",
      "        0.82623475,  0.83095702,  0.83563731,  0.84027637,  0.84487491,\n",
      "        0.84943363,  0.85395323,  0.85843435,  0.86287767,  0.8672838 ,\n",
      "        0.87165336,  0.87598696,  0.88028519,  0.8845486 ,  0.88877778,\n",
      "        0.89297326,  0.89713557,  0.90126523,  0.90536276,  0.90942864,\n",
      "        0.91346337,  0.91746741,  0.92144124,  0.9253853 ,  0.92930003,\n",
      "        0.93318588,  0.93704325,  0.94087258,  0.94467425,  0.94844867,\n",
      "        0.95219623,  0.9559173 ,  0.95961226,  0.96328147,  0.96692528,\n",
      "        0.97054406,  0.97413813,  0.97770783,  0.9812535 ,  0.98477545,\n",
      "        0.988274  ,  0.99174945,  0.99520211,  0.99863229,  1.00204025,\n",
      "        1.00542631,  1.00879072,  1.01213378,  1.01545574,  1.01875688,\n",
      "        1.02203745,  1.0252977 ,  1.02853789,  1.03175827,  1.03495906,\n",
      "        1.03814052,  1.04130287,  1.04444634,  1.04757115,  1.05067753,\n",
      "        1.05376569,  1.05683584,  1.05988819,  1.06292295,  1.06594032,\n",
      "        1.0689405 ,  1.07192367,  1.07489004,  1.0778398 ,  1.08077312,\n",
      "        1.08369018,  1.08659118,  1.08947628,  1.09234566,  1.09519948,\n",
      "        1.09803792,  1.10086114,  1.10366931,  1.10646257,  1.1092411 ,\n",
      "        1.11200504,  1.11475455,  1.11748977,  1.12021086,  1.12291796,\n",
      "        1.12561121,  1.12829075,  1.13095673,  1.13360928,  1.13624853,\n",
      "        1.13887462,  1.14148767,  1.14408782,  1.1466752 ,  1.14924992,\n",
      "        1.15181211,  1.15436189,  1.15689939,  1.15942472,  1.16193799,\n",
      "        1.16443932,  1.16692883,  1.16940662,  1.1718728 ,  1.17432749,\n",
      "        1.17677078,  1.17920279,  1.18162362,  1.18403337,  1.18643215,\n",
      "        1.18882004,  1.19119715,  1.19356358,  1.19591942,  1.19826477,\n",
      "        1.20059971,  1.20292435,  1.20523877,  1.20754306,  1.20983731,\n",
      "        1.2121216 ,  1.21439603,  1.21666068,  1.21891563,  1.22116096,\n",
      "        1.22339675,  1.2256231 ,  1.22784007,  1.23004774,  1.2322462 ,\n",
      "        1.23443551,  1.23661576,  1.23878702,  1.24094936,  1.24310286,\n",
      "        1.24524758,  1.24738361,  1.249511  ,  1.25162984,  1.25374018,\n",
      "        1.2558421 ,  1.25793566,  1.26002092,  1.26209797,  1.26416685,\n",
      "        1.26622763,  1.26828038,  1.27032516,  1.27236202,  1.27439104,\n",
      "        1.27641227,  1.27842577,  1.2804316 ,  1.28242981,  1.28442048,\n",
      "        1.28640364,  1.28837936,  1.2903477 ,  1.2923087 ,  1.29426243,\n",
      "        1.29620894,  1.29814828,  1.3000805 ,  1.30200565,  1.3039238 ,\n",
      "        1.30583498,  1.30773924,  1.30963665,  1.31152724,  1.31341107,\n",
      "        1.31528818,  1.31715863,  1.31902245,  1.3208797 ,  1.32273042,\n",
      "        1.32457466,  1.32641246,  1.32824387,  1.33006893,  1.33188768,\n",
      "        1.33370018,  1.33550646,  1.33730656,  1.33910053,  1.3408884 ,\n",
      "        1.34267023,  1.34444605,  1.3462159 ,  1.34797982,  1.34973785,\n",
      "        1.35149003,  1.35323639,  1.35497699,  1.35671185,  1.35844101,\n",
      "        1.36016451,  1.36188239,  1.36359469,  1.36530143,  1.36700265,\n",
      "        1.3686984 ,  1.37038871,  1.3720736 ,  1.37375312,  1.37542731,\n",
      "        1.37709618,  1.37875978,  1.38041814,  1.3820713 ,  1.38371928,\n",
      "        1.38536212,  1.38699985,  1.3886325 ,  1.3902601 ,  1.39188269,\n",
      "        1.39350029,  1.39511294,  1.39672067,  1.3983235 ,  1.39992146,\n",
      "        1.40151459,  1.40310292,  1.40468647,  1.40626527,  1.40783935,\n",
      "        1.40940874,  1.41097346,  1.41253355,  1.41408903,  1.41563993,\n",
      "        1.41718627,  1.41872809,  1.4202654 ,  1.42179824,  1.42332663,\n",
      "        1.42485059,  1.42637016,  1.42788535,  1.4293962 ,  1.43090272,\n",
      "        1.43240494,  1.4339029 ,  1.4353966 ,  1.43688607,  1.43837135,\n",
      "        1.43985244,  1.44132939,  1.4428022 ,  1.4442709 ,  1.44573552,\n",
      "        1.44719607,  1.44865258,  1.45010508,  1.45155358,  1.45299811,\n",
      "        1.45443868,  1.45587532,  1.45730806,  1.4587369 ,  1.46016188,\n",
      "        1.46158302,  1.46300032,  1.46441383,  1.46582354,  1.4672295 ,\n",
      "        1.46863171,  1.47003019,  1.47142497,  1.47281607,  1.4742035 ,\n",
      "        1.47558728,  1.47696744,  1.47834399,  1.47971695,  1.48108634,\n",
      "        1.48245217,  1.48381447,  1.48517326,  1.48652855,  1.48788036,\n",
      "        1.48922871,  1.49057361,  1.49191509,  1.49325315,  1.49458783,\n",
      "        1.49591913,  1.49724708,  1.49857168,  1.49989296,  1.50121093,\n",
      "        1.50252561,  1.50383702,  1.50514516,  1.50645007,  1.50775175,\n",
      "        1.50905022,  1.5103455 ,  1.5116376 ,  1.51292653,  1.51421232,\n",
      "        1.51549497,  1.51677451,  1.51805094,  1.51932429,  1.52059457,\n",
      "        1.52186179,  1.52312597,  1.52438712,  1.52564525,  1.52690039,\n",
      "        1.52815254,  1.52940172,  1.53064794,  1.53189122,  1.53313158,\n",
      "        1.53436902,  1.53560355,  1.5368352 ,  1.53806397,  1.53928989,\n",
      "        1.54051295,  1.54173318,  1.54295059,  1.54416519,  1.545377  ,\n",
      "        1.54658602,  1.54779228,  1.54899577,  1.55019653,  1.55139455,\n",
      "        1.55258985,  1.55378244,  1.55497234,  1.55615955,  1.55734409,\n",
      "        1.55852598,  1.55970522,  1.56088182,  1.5620558 ,  1.56322717,\n",
      "        1.56439593,  1.56556211,  1.56672571,  1.56788675,  1.56904523,\n",
      "        1.57020117,  1.57135457,  1.57250546,  1.57365383,  1.57479971,\n",
      "        1.57594309,  1.577084  ,  1.57822245,  1.57935843,  1.58049197,\n",
      "        1.58162308,  1.58275176,  1.58387803,  1.58500189,  1.58612336,\n",
      "        1.58724244,  1.58835915,  1.5894735 ,  1.5905855 ,  1.59169515,\n",
      "        1.59280247,  1.59390746,  1.59501014,  1.59611052,  1.5972086 ,\n",
      "        1.59830439,  1.59939791,  1.60048917,  1.60157816,  1.60266491,\n",
      "        1.60374942,  1.6048317 ,  1.60591176,  1.60698961,  1.60806526,\n",
      "        1.60913871,  1.61020998,  1.61127908,  1.61234601,  1.61341078,\n",
      "        1.6144734 ,  1.61553389,  1.61659224,  1.61764847,  1.61870258,\n",
      "        1.61975459,  1.6208045 ,  1.62185232,  1.62289806,  1.62394172,\n",
      "        1.62498332,  1.62602287,  1.62706036,  1.62809582,  1.62912925,\n",
      "        1.63016065,  1.63119003]), array([    6, 22879,  7787,  3459,  3140,  2613,  2241,  2132,  1793,\n",
      "        1645,  1555,  1505,  1372,  1234,  1094,  1019,   986,   884,\n",
      "         802,   732,   639,   620,   550,   498,   455,   415,   385,\n",
      "         384,   320,   310,   277,   299,   259,   245,   227,   206,\n",
      "         192,   189,   162,   173,   165,   150,   145,   120,   107,\n",
      "         121,   105,   100,    97,    85,   109,    96,   107,    94,\n",
      "          97,   104,    93,    94,    92,    91,    94,    98,    97,\n",
      "          89,   142,   150,   153,   162,   159,   148,   159,   157,\n",
      "         153,   167,   160,   169,   163,   163,   160,   158,   155,\n",
      "          84,    79,    74,    87,    84,    76,    89,    85,    89,\n",
      "          95,    94,    88,    90,    96,    95,   105,   106,   118,\n",
      "         112,   131,   286,   292,   286,   296,   261,   290,   263,\n",
      "         260,   271,   264,   268,   270,   256,   275,   282,   281,\n",
      "         294,   277,   302,   286,   289,   280,   283,   279,   297,\n",
      "         289,   294,   293,   298,   286,   287,   295,   298,   278,\n",
      "         310,   285,   270,   289,   295,   297,   314,   320,   317,\n",
      "         328,   282,   327,   313,   305,   306,   259,    74,    73,\n",
      "          75,    76,    73,    65,    69,    93,    82,    82,    65,\n",
      "          60,    72,    61,    71,    62,    65,    66,    60,    70,\n",
      "          83,    86,    83,    73,    80,    75,    80,    87,    87,\n",
      "          91,    84,    91,    77,    86,    87,    87,    89,   100,\n",
      "         110,   136,   118,   114,   104,   121,   128,   101,   106,\n",
      "         126,   147,   155,   254,   248,   228,   202,   211,   207,\n",
      "         182,   192,   188,   165,   197,   178,   177,   171,   184,\n",
      "         168,   186,   168,   154,   172,   161,   149,   167,   170,\n",
      "         185,   200,   220,   236,   229,   246,   259,   247,   243,\n",
      "         268,   237,   250,   233,   240,   223,   207,   219,   211,\n",
      "         230,   193,   213,   200,   221,   200,   217,   189,   209,\n",
      "         175,   205,   221,   199,   190,   214,   219,   200,   221,\n",
      "         206,   235,   237,   217,   210,   239,   226,   200,   224,\n",
      "         189,   197,   240,   223,   213,   231,   226,   203,   209,\n",
      "         187,   188,   164,   181,   184,   203,   183,   169,   180,\n",
      "         150,   173,   162,   148,   162,   145,   153,   161,   203,\n",
      "         190,   173,   170,   163,    15,    11,    12,    13,    13,\n",
      "          13,    12,    13,    11,    11,    11,    12,    12,    11,\n",
      "          12,    13,    13,    12,    10,    10,    12,    12,     8,\n",
      "          10,     7,     8,     8,     8,     7,     7,     7,     6,\n",
      "           7,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     5,     6,     5,     5,     6,     7,     7,     6,\n",
      "           6,     6,     5,     6,     7,     6,     7,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     5,\n",
      "           7,     7,     6,     7,     7,     6,     6,     4,     5,\n",
      "           5,     5,     6,     7,     7,     7,     7,     6,     6,\n",
      "           6,     7,     6,     7,     6,     6,     5,     6,     6,\n",
      "           6,     6,     5,     6,     6,     5,     5,     5,     4,\n",
      "           5,     5,     5,     5,     6,     6,     6,     6,     6,\n",
      "           5,     5,     5,     6,     6,     5,     5,     5,     5,\n",
      "           6,     5,     8,     7,     9,     5,     7,     7,     6,\n",
      "          12,     5,     7,    13,    10,    19,     6,    10,    18,\n",
      "           8,    11,     8,     8,     5,     9,     8,    11,     9,\n",
      "           7,     7,     8,     8,     7,     6,     4,     6,     5,\n",
      "           6,     4,     4,     4,     5,     5,     5,     6,     9,\n",
      "          11,    15,    11,    18,    18,    18,     7,     7,     7,\n",
      "           9,     5,     4,     4,     4,     5,     5,     5,     5,\n",
      "           4,     6,     5,     4,     4,     4,     3,     4,     4,\n",
      "           3,     2,     3,     6,    31,   243,   307,  1437],\n",
      "      dtype=int64))\n",
      "\n",
      "srv_count\n",
      "(array([-1.82279815, -1.26474348, -0.93830242, -0.70668881, -0.52703533,\n",
      "       -0.38024775, -0.25614062, -0.14863414, -0.0538067 ,  0.03101934,\n",
      "        0.10775382,  0.17780692,  0.24224952,  0.30191405,  0.35746039,\n",
      "        0.40942053,  0.45822958,  0.50424797,  0.54777764,  0.58907401,\n",
      "        0.6283551 ,  0.66580849,  0.70159673,  0.73586159,  0.76872748,\n",
      "        0.80030419,  0.83068903,  0.85996872,  0.88822083,  0.91551506,\n",
      "        0.94191424,  0.9674752 ,  0.99224955,  1.01628425,  1.03962219,\n",
      "        1.06230264,  1.08436163,  1.10583231,  1.12674524,  1.14712868,\n",
      "        1.16700877,  1.18640977,  1.20535424,  1.22386316,  1.24195612,\n",
      "        1.2596514 ,  1.2769661 ,  1.29391626,  1.3105169 ,  1.32678215,\n",
      "        1.3427253 ,  1.35835886,  1.37369461,  1.3887437 ,  1.40351664,\n",
      "        1.41802338,  1.43227336,  1.4462755 ,  1.46003828,  1.47356973,\n",
      "        1.48687752,  1.49996891,  1.51285083,  1.52552987,  1.53801233,\n",
      "        1.55030422,  1.56241125,  1.57433892,  1.58609245,  1.59767686,\n",
      "        1.60909695,  1.62035731,  1.63146235,  1.6424163 ,  1.65322321,\n",
      "        1.66388698,  1.67441135,  1.68479991,  1.69505614,  1.70518335,\n",
      "        1.71518475,  1.72506344,  1.73482238,  1.74446444,  1.75399239,\n",
      "        1.76340891,  1.77271656,  1.78191783,  1.79101513,  1.80001079,\n",
      "        1.80890704,  1.81770606,  1.82640996,  1.83502077,  1.84354045,\n",
      "        1.85197093,  1.86031403,  1.86857157,  1.87674527,  1.88483682,\n",
      "        1.89284786,  1.90077997,  1.90863469,  1.91641353,  1.92411792,\n",
      "        1.93174928,  1.93930898,  1.94679837,  1.95421872,  1.96157131,\n",
      "        1.96885735,  1.97607805,  1.98323457,  1.99032803,  1.99735954,\n",
      "        2.00433017,  2.01124097,  2.01809295,  2.0248871 ,  2.0316244 ,\n",
      "        2.03830579,  2.04493219,  2.05150449,  2.05802358,  2.0644903 ,\n",
      "        2.07090549,  2.07726998,  2.08358454,  2.08984996,  2.096067  ,\n",
      "        2.1022364 ,  2.10835889,  2.11443516,  2.12046592,  2.12645184,\n",
      "        2.13239359,  2.1382918 ,  2.14414712,  2.14996016,  2.15573153,\n",
      "        2.16146182,  2.16715162,  2.17280149,  2.17841198,  2.18398365,\n",
      "        2.18951702,  2.19501262,  2.20047097,  2.20589256,  2.21127788,\n",
      "        2.21662742,  2.22194165,  2.22722103,  2.23246601,  2.23767705,\n",
      "        2.24285458,  2.24799902,  2.2531108 ,  2.25819033,  2.26323802,\n",
      "        2.26825425,  2.27323942,  2.27819391,  2.28311811,  2.28801236,\n",
      "        2.29287705,  2.29771251,  2.30251911,  2.30729718,  2.31204706,\n",
      "        2.31676909,  2.32146358,  2.32613085,  2.33077123,  2.33538501,\n",
      "        2.3399725 ,  2.344534  ,  2.3490698 ,  2.35358019,  2.35806546,\n",
      "        2.36252587,  2.36696171,  2.37137324,  2.37576073,  2.38012444,\n",
      "        2.38446463,  2.38878155,  2.39307544,  2.39734655,  2.40159512,\n",
      "        2.40582139,  2.41002559,  2.41420796,  2.4183687 ,  2.42250806,\n",
      "        2.42662624,  2.43072346,  2.43479994,  2.43885588,  2.44289149,\n",
      "        2.44690698,  2.45090253,  2.45487835,  2.45883464,  2.46277158,\n",
      "        2.46668936,  2.47058817,  2.47446819,  2.47832961,  2.48217259,\n",
      "        2.48599731,  2.48980395,  2.49359267,  2.49736365,  2.50111705,\n",
      "        2.50485304,  2.50857176,  2.51227339,  2.51595808,  2.51962598,\n",
      "        2.52327724,  2.52691202,  2.53053047,  2.53413272,  2.53771893,\n",
      "        2.54128924,  2.54484378,  2.5483827 ,  2.55190613,  2.55541421,\n",
      "        2.55890707,  2.56238484,  2.56584765,  2.56929564,  2.57272891,\n",
      "        2.57614761,  2.57955186,  2.58294177,  2.58631747,  2.58967907,\n",
      "        2.5930267 ,  2.59636046,  2.59968048,  2.60298686,  2.60627972,\n",
      "        2.60955916,  2.6128253 ,  2.61607825,  2.6193181 ,  2.62254497,\n",
      "        2.62575896,  2.62896016,  2.63214869,  2.63532465,  2.63848812,\n",
      "        2.64163921,  2.64477801,  2.64790463,  2.65101915,  2.65412167,\n",
      "        2.65721228,  2.66029107,  2.66335813,  2.66641355,  2.66945742,\n",
      "        2.67248983,  2.67551086,  2.67852059,  2.68151911,  2.68450651,\n",
      "        2.68748287,  2.69044826,  2.69340276,  2.69634647,  2.69927945,\n",
      "        2.70220179,  2.70511356,  2.70801483,  2.71090569,  2.7137862 ,\n",
      "        2.71665644,  2.71951649,  2.72236642,  2.72520629,  2.72803618,\n",
      "        2.73085616,  2.73366629,  2.73646665,  2.7392573 ,  2.74203832,\n",
      "        2.74480976,  2.74757169,  2.75032418,  2.75306729,  2.75580109,\n",
      "        2.75852564,  2.761241  ,  2.76394723,  2.76664439,  2.76933255,\n",
      "        2.77201176,  2.77468209,  2.77734359,  2.77999632,  2.78264033,\n",
      "        2.7852757 ,  2.78790246,  2.79052068,  2.79313042,  2.79573172,\n",
      "        2.79832465,  2.80090925,  2.80348558,  2.80605369,  2.80861364,\n",
      "        2.81116547,  2.81370924,  2.816245  ,  2.8187728 ,  2.82129269,\n",
      "        2.82380471,  2.82630892,  2.82880536,  2.83129409,  2.83377515,\n",
      "        2.83624858,  2.83871444,  2.84117277,  2.84362362,  2.84606703,\n",
      "        2.84850305,  2.85093171,  2.85335308,  2.85576718,  2.85817407,\n",
      "        2.86057378,  2.86296636,  2.86535185,  2.86773029,  2.87010173,\n",
      "        2.87246621,  2.87482376,  2.87717442,  2.87951825,  2.88185527,\n",
      "        2.88418552,  2.88650905,  2.8888259 ,  2.89113609,  2.89343968,\n",
      "        2.89573669,  2.89802717,  2.90031115,  2.90258867,  2.90485976,\n",
      "        2.90712447,  2.90938283,  2.91163486,  2.91388062,  2.91612013,\n",
      "        2.91835342,  2.92058054,  2.92280151,  2.92501638,  2.92722517,\n",
      "        2.92942791,  2.93162465,  2.9338154 ,  2.93600022,  2.93817911,\n",
      "        2.94035213,  2.9425193 ,  2.94468065,  2.94683622,  2.94898602,\n",
      "        2.95113011,  2.9532685 ,  2.95540122,  2.95752831,  2.95964979,\n",
      "        2.9617657 ,  2.96387606,  2.96598091,  2.96808026,  2.97017416,\n",
      "        2.97226263,  2.97434569,  2.97642337,  2.97849571,  2.98056273,\n",
      "        2.98262445,  2.98468091,  2.98673213,  2.98877813,  2.99081895,\n",
      "        2.99285461,  2.99488513,  2.99691055,  2.99893089,  3.00094616,\n",
      "        3.00295641,  3.00496165,  3.0069619 ,  3.0089572 ,  3.01094757,\n",
      "        3.01293302,  3.0149136 ,  3.01688931,  3.01886019,  3.02082625,\n",
      "        3.02278753,  3.02474403,  3.0266958 ,  3.02864284,  3.03058519,\n",
      "        3.03252286,  3.03445589,  3.03638428,  3.03830806,  3.04022726,\n",
      "        3.04214189,  3.04405198,  3.04595755,  3.04785862,  3.04975521,\n",
      "        3.05164734,  3.05353504,  3.05541832,  3.05729721,  3.05917172,\n",
      "        3.06104188,  3.0629077 ,  3.06476921,  3.06662643,  3.06847937,\n",
      "        3.07032806,  3.07217251,  3.07401275,  3.07584878,  3.07768065,\n",
      "        3.07950835,  3.08133191,  3.08315135,  3.08496669,  3.08677795,\n",
      "        3.08858514,  3.09038828,  3.09218739,  3.09398249,  3.0957736 ,\n",
      "        3.09756073,  3.09934391,  3.10112314,  3.10289845,  3.10466986,\n",
      "        3.10643737,  3.10820101,  3.1099608 ,  3.11171675,  3.11346888,\n",
      "        3.1152172 ,  3.11696174,  3.1187025 ,  3.12043951,  3.12217278,\n",
      "        3.12390232,  3.12562816,  3.1273503 ,  3.12906877,  3.13078358,\n",
      "        3.13249475,  3.13420228,  3.13590621,  3.13760653,  3.13930327,\n",
      "        3.14099644,  3.14268606,  3.14437214,  3.14605469,  3.14773374,\n",
      "        3.14940929,  3.15108137,  3.15274997,  3.15441513,  3.15607685,\n",
      "        3.15773515,  3.15939003,  3.16104153,  3.16268964,  3.16433438,\n",
      "        3.16597578,  3.16761383,  3.16924856,  3.17087997,  3.17250809,\n",
      "        3.17413292,  3.17575447,  3.17737277,  3.17898782,  3.18059964,\n",
      "        3.18220824,  3.18541582,  3.18701483,  3.19337932,  3.19496261,\n",
      "        3.19654279,  3.19811987,  3.19969388]), array([    6, 18651, 11352,  6127,  5407,  4541,  4075,  3949,  3652,\n",
      "        3496,  3228,  3269,  3098,  2920,  2936,  2776,  2856,  2855,\n",
      "        2887,  2986,  3138,  1131,  1076,  1029,  1060,  1031,   456,\n",
      "         456,   403,   381,   369,   382,   352,   325,   320,   287,\n",
      "         267,   268,   230,   234,   220,   219,   200,   175,   190,\n",
      "         162,   157,   158,   139,   126,   115,   115,   123,    58,\n",
      "          67,    37,    43,    49,    31,    39,    34,    37,    37,\n",
      "          31,    27,    35,    34,    32,    35,    30,    27,    30,\n",
      "          27,    24,    19,    27,    22,    22,    20,    21,    17,\n",
      "          24,    25,    21,    21,    24,    19,    20,    18,    27,\n",
      "          23,    15,    16,    17,    13,    23,    22,    23,    19,\n",
      "          17,    10,    13,    14,    10,    31,    30,    29,    35,\n",
      "          34,    33,    30,    32,    31,    33,    31,    33,    29,\n",
      "          28,    29,    29,    30,    29,    35,    31,    26,    33,\n",
      "          27,    29,    30,    27,    31,    27,    29,    29,    25,\n",
      "          28,    25,    30,    30,    34,    31,    37,    35,    33,\n",
      "          90,    89,    90,    93,    88,    88,    96,    89,    86,\n",
      "          97,    89,    97,    95,    87,    90,    85,    75,    33,\n",
      "          33,    26,    27,    30,    30,    29,    28,    27,    28,\n",
      "          29,    28,    28,    29,    28,    25,    27,    28,    28,\n",
      "          25,    29,    24,    27,    28,    21,    24,    28,    21,\n",
      "          27,    26,    27,    25,    26,    29,    28,    26,    32,\n",
      "          30,    29,    30,    31,    29,    25,    27,    23,    25,\n",
      "          22,    24,    21,    22,    23,    22,    21,    20,    22,\n",
      "          20,    24,    24,    25,    25,    25,    22,    27,    20,\n",
      "          26,    24,    26,    26,    27,    24,    27,    22,    24,\n",
      "          23,    24,    25,    25,    21,    24,    22,    12,    14,\n",
      "          12,    11,    11,    12,    11,    13,    13,    13,    11,\n",
      "          13,    13,    14,    12,    13,    12,    10,    11,    10,\n",
      "          12,    12,    12,    11,    11,    12,    13,    12,    12,\n",
      "          11,    14,    12,    13,    13,    12,    11,    11,    11,\n",
      "          11,    10,    10,    10,    11,    10,    10,     9,     9,\n",
      "          11,     9,    10,    11,    10,    10,    10,    11,    10,\n",
      "          11,    10,    10,    11,    10,    10,    10,     9,    11,\n",
      "          10,     9,    11,    10,    10,    10,    10,     9,     8,\n",
      "          10,    10,    10,     8,     6,     6,     7,     7,     3,\n",
      "           5,     3,     3,     3,     3,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     3,     3,     2,\n",
      "           2,     2,     1,     3,     3,     3,     3,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           3,     3,     3,     3,     3,     3,     2,     1,     2,\n",
      "           2,     2,     2,     3,     3,     3,     3,     2,     2,\n",
      "           2,     3,     3,     3,     2,     2,     2,     2,     2,\n",
      "           2,     2,     1,     2,     2,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     1,     1,     1,     1,\n",
      "           2,     1,     4,     3,     5,     1,     3,     3,     2,\n",
      "           8,     1,     5,     9,     7,    16,     5,     6,    15,\n",
      "           4,     7,     5,     6,     3,     6,     5,     9,     6,\n",
      "           4,     4,     7,     5,     4,     3,     1,     3,     3,\n",
      "           3,     1,     1,     1,     2,     2,     2,     3,     6,\n",
      "           8,    11,     8,    16,    15,    16,     5,     5,     4,\n",
      "           6,     3,     1,     1,     1,     2,     2,     2,     2,\n",
      "           1,     3,     2,     1,     1,     1,     1,     1,     3,\n",
      "          11,    49,   158,  1010], dtype=int64))\n",
      "\n",
      "same_srv_rate\n",
      "(array([0, 1, 2]), array([20058, 18559, 76557], dtype=int64))\n",
      "\n",
      "diff_srv_rate\n",
      "(array([0, 1, 2]), array([77576, 18954, 18644], dtype=int64))\n",
      "\n",
      "dst_host_count\n",
      "(array([0.        , 0.00392157, 0.00784314, 0.01176471, 0.01568627,\n",
      "       0.01960784, 0.02352941, 0.02745098, 0.03137255, 0.03529412,\n",
      "       0.03921569, 0.04313725, 0.04705882, 0.05098039, 0.05490196,\n",
      "       0.05882353, 0.0627451 , 0.06666667, 0.07058824, 0.0745098 ,\n",
      "       0.07843137, 0.08235294, 0.08627451, 0.09019608, 0.09411765,\n",
      "       0.09803922, 0.10196078, 0.10588235, 0.10980392, 0.11372549,\n",
      "       0.11764706, 0.12156863, 0.1254902 , 0.12941176, 0.13333333,\n",
      "       0.1372549 , 0.14117647, 0.14509804, 0.14901961, 0.15294118,\n",
      "       0.15686275, 0.16078431, 0.16470588, 0.16862745, 0.17254902,\n",
      "       0.17647059, 0.18039216, 0.18431373, 0.18823529, 0.19215686,\n",
      "       0.19607843, 0.2       , 0.20392157, 0.20784314, 0.21176471,\n",
      "       0.21568627, 0.21960784, 0.22352941, 0.22745098, 0.23137255,\n",
      "       0.23529412, 0.23921569, 0.24313725, 0.24705882, 0.25098039,\n",
      "       0.25490196, 0.25882353, 0.2627451 , 0.26666667, 0.27058824,\n",
      "       0.2745098 , 0.27843137, 0.28235294, 0.28627451, 0.29019608,\n",
      "       0.29411765, 0.29803922, 0.30196078, 0.30588235, 0.30980392,\n",
      "       0.31372549, 0.31764706, 0.32156863, 0.3254902 , 0.32941176,\n",
      "       0.33333333, 0.3372549 , 0.34117647, 0.34509804, 0.34901961,\n",
      "       0.35294118, 0.35686275, 0.36078431, 0.36470588, 0.36862745,\n",
      "       0.37254902, 0.37647059, 0.38039216, 0.38431373, 0.38823529,\n",
      "       0.39215686, 0.39607843, 0.4       , 0.40392157, 0.40784314,\n",
      "       0.41176471, 0.41568627, 0.41960784, 0.42352941, 0.42745098,\n",
      "       0.43137255, 0.43529412, 0.43921569, 0.44313725, 0.44705882,\n",
      "       0.45098039, 0.45490196, 0.45882353, 0.4627451 , 0.46666667,\n",
      "       0.47058824, 0.4745098 , 0.47843137, 0.48235294, 0.48627451,\n",
      "       0.49019608, 0.49411765, 0.49803922, 0.50196078, 0.50588235,\n",
      "       0.50980392, 0.51372549, 0.51764706, 0.52156863, 0.5254902 ,\n",
      "       0.52941176, 0.53333333, 0.5372549 , 0.54117647, 0.54509804,\n",
      "       0.54901961, 0.55294118, 0.55686275, 0.56078431, 0.56470588,\n",
      "       0.56862745, 0.57254902, 0.57647059, 0.58039216, 0.58431373,\n",
      "       0.58823529, 0.59215686, 0.59607843, 0.6       , 0.60392157,\n",
      "       0.60784314, 0.61176471, 0.61568627, 0.61960784, 0.62352941,\n",
      "       0.62745098, 0.63137255, 0.63529412, 0.63921569, 0.64313725,\n",
      "       0.64705882, 0.65098039, 0.65490196, 0.65882353, 0.6627451 ,\n",
      "       0.66666667, 0.67058824, 0.6745098 , 0.67843137, 0.68235294,\n",
      "       0.68627451, 0.69019608, 0.69411765, 0.69803922, 0.70196078,\n",
      "       0.70588235, 0.70980392, 0.71372549, 0.71764706, 0.72156863,\n",
      "       0.7254902 , 0.72941176, 0.73333333, 0.7372549 , 0.74117647,\n",
      "       0.74509804, 0.74901961, 0.75294118, 0.75686275, 0.76078431,\n",
      "       0.76470588, 0.76862745, 0.77254902, 0.77647059, 0.78039216,\n",
      "       0.78431373, 0.78823529, 0.79215686, 0.79607843, 0.8       ,\n",
      "       0.80392157, 0.80784314, 0.81176471, 0.81568627, 0.81960784,\n",
      "       0.82352941, 0.82745098, 0.83137255, 0.83529412, 0.83921569,\n",
      "       0.84313725, 0.84705882, 0.85098039, 0.85490196, 0.85882353,\n",
      "       0.8627451 , 0.86666667, 0.87058824, 0.8745098 , 0.87843137,\n",
      "       0.88235294, 0.88627451, 0.89019608, 0.89411765, 0.89803922,\n",
      "       0.90196078, 0.90588235, 0.90980392, 0.91372549, 0.91764706,\n",
      "       0.92156863, 0.9254902 , 0.92941176, 0.93333333, 0.9372549 ,\n",
      "       0.94117647, 0.94509804, 0.94901961, 0.95294118, 0.95686275,\n",
      "       0.96078431, 0.96470588, 0.96862745, 0.97254902, 0.97647059,\n",
      "       0.98039216, 0.98431373, 0.98823529, 0.99215686, 0.99607843,\n",
      "       1.        ]), array([    3,  2935,  2647,  1199,  1154,   695,   668,   609,   562,\n",
      "         550,   520,   517,   497,   440,   412,   396,   367,   357,\n",
      "         365,   359,   362,   346,   346,   318,   312,   323,   313,\n",
      "         321,   292,   277,   270,   294,   272,   254,   274,   261,\n",
      "         281,   255,   240,   246,   238,   256,   238,   233,   214,\n",
      "         226,   198,   208,   194,   207,   219,   214,   206,   198,\n",
      "         193,   210,   187,   194,   199,   203,   196,   175,   206,\n",
      "         164,   176,   165,   172,   181,   165,   168,   166,   166,\n",
      "         196,   153,   179,   168,   166,   166,   160,   163,   132,\n",
      "         158,   148,   154,   157,   152,   128,   137,   145,   132,\n",
      "         129,   135,   140,   128,   130,   129,   134,   132,   131,\n",
      "         137,   122,   123,   124,   134,   127,   120,   134,   113,\n",
      "         144,   130,   114,   130,   131,   127,   139,   130,   134,\n",
      "         110,   120,   120,   132,   113,   115,   118,   122,   130,\n",
      "         108,   119,   121,   107,   115,   105,   102,   106,   108,\n",
      "         105,   115,   111,    99,    94,   103,   109,   114,   110,\n",
      "         119,    96,   101,   118,    99,    98,   104,    95,   118,\n",
      "          99,    99,    92,   113,    96,    97,   100,    87,   109,\n",
      "         102,   109,    94,   100,   108,    84,   104,   117,   101,\n",
      "          93,    95,   100,    94,   103,    96,   102,    99,    88,\n",
      "          81,    96,    96,    81,    95,    80,    88,    98,    92,\n",
      "          95,    89,    94,    81,    97,    92,    90,    85,    96,\n",
      "          99,    79,    82,    82,    90,    99,    81,    73,    68,\n",
      "          87,    91,    81,    81,    94,    86,    75,    84,    85,\n",
      "          78,    83,    75,    75,    75,    70,    82,    81,    66,\n",
      "          75,    63,    71,    75,    75,    66,    73,    79,    56,\n",
      "          78,    74,    64,    72,    76,    81,    77,    70,    70,\n",
      "          62,    76,    77,    74,    78,    73,    67,    78,    75,\n",
      "          66,    73,    56, 68797], dtype=int64))\n",
      "\n",
      "dst_host_srv_count\n",
      "(array([0.        , 0.00392157, 0.00784314, 0.01176471, 0.01568627,\n",
      "       0.01960784, 0.02352941, 0.02745098, 0.03137255, 0.03529412,\n",
      "       0.03921569, 0.04313725, 0.04705882, 0.05098039, 0.05490196,\n",
      "       0.05882353, 0.0627451 , 0.06666667, 0.07058824, 0.0745098 ,\n",
      "       0.07843137, 0.08235294, 0.08627451, 0.09019608, 0.09411765,\n",
      "       0.09803922, 0.10196078, 0.10588235, 0.10980392, 0.11372549,\n",
      "       0.11764706, 0.12156863, 0.1254902 , 0.12941176, 0.13333333,\n",
      "       0.1372549 , 0.14117647, 0.14509804, 0.14901961, 0.15294118,\n",
      "       0.15686275, 0.16078431, 0.16470588, 0.16862745, 0.17254902,\n",
      "       0.17647059, 0.18039216, 0.18431373, 0.18823529, 0.19215686,\n",
      "       0.19607843, 0.2       , 0.20392157, 0.20784314, 0.21176471,\n",
      "       0.21568627, 0.21960784, 0.22352941, 0.22745098, 0.23137255,\n",
      "       0.23529412, 0.23921569, 0.24313725, 0.24705882, 0.25098039,\n",
      "       0.25490196, 0.25882353, 0.2627451 , 0.26666667, 0.27058824,\n",
      "       0.2745098 , 0.27843137, 0.28235294, 0.28627451, 0.29019608,\n",
      "       0.29411765, 0.29803922, 0.30196078, 0.30588235, 0.30980392,\n",
      "       0.31372549, 0.31764706, 0.32156863, 0.3254902 , 0.32941176,\n",
      "       0.33333333, 0.3372549 , 0.34117647, 0.34509804, 0.34901961,\n",
      "       0.35294118, 0.35686275, 0.36078431, 0.36470588, 0.36862745,\n",
      "       0.37254902, 0.37647059, 0.38039216, 0.38431373, 0.38823529,\n",
      "       0.39215686, 0.39607843, 0.4       , 0.40392157, 0.40784314,\n",
      "       0.41176471, 0.41568627, 0.41960784, 0.42352941, 0.42745098,\n",
      "       0.43137255, 0.43529412, 0.43921569, 0.44313725, 0.44705882,\n",
      "       0.45098039, 0.45490196, 0.45882353, 0.4627451 , 0.46666667,\n",
      "       0.47058824, 0.4745098 , 0.47843137, 0.48235294, 0.48627451,\n",
      "       0.49019608, 0.49411765, 0.49803922, 0.50196078, 0.50588235,\n",
      "       0.50980392, 0.51372549, 0.51764706, 0.52156863, 0.5254902 ,\n",
      "       0.52941176, 0.53333333, 0.5372549 , 0.54117647, 0.54509804,\n",
      "       0.54901961, 0.55294118, 0.55686275, 0.56078431, 0.56470588,\n",
      "       0.56862745, 0.57254902, 0.57647059, 0.58039216, 0.58431373,\n",
      "       0.58823529, 0.59215686, 0.59607843, 0.6       , 0.60392157,\n",
      "       0.60784314, 0.61176471, 0.61568627, 0.61960784, 0.62352941,\n",
      "       0.62745098, 0.63137255, 0.63529412, 0.63921569, 0.64313725,\n",
      "       0.64705882, 0.65098039, 0.65490196, 0.65882353, 0.6627451 ,\n",
      "       0.66666667, 0.67058824, 0.6745098 , 0.67843137, 0.68235294,\n",
      "       0.68627451, 0.69019608, 0.69411765, 0.69803922, 0.70196078,\n",
      "       0.70588235, 0.70980392, 0.71372549, 0.71764706, 0.72156863,\n",
      "       0.7254902 , 0.72941176, 0.73333333, 0.7372549 , 0.74117647,\n",
      "       0.74509804, 0.74901961, 0.75294118, 0.75686275, 0.76078431,\n",
      "       0.76470588, 0.76862745, 0.77254902, 0.77647059, 0.78039216,\n",
      "       0.78431373, 0.78823529, 0.79215686, 0.79607843, 0.8       ,\n",
      "       0.80392157, 0.80784314, 0.81176471, 0.81568627, 0.81960784,\n",
      "       0.82352941, 0.82745098, 0.83137255, 0.83529412, 0.83921569,\n",
      "       0.84313725, 0.84705882, 0.85098039, 0.85490196, 0.85882353,\n",
      "       0.8627451 , 0.86666667, 0.87058824, 0.8745098 , 0.87843137,\n",
      "       0.88235294, 0.88627451, 0.89019608, 0.89411765, 0.89803922,\n",
      "       0.90196078, 0.90588235, 0.90980392, 0.91372549, 0.91764706,\n",
      "       0.92156863, 0.9254902 , 0.92941176, 0.93333333, 0.9372549 ,\n",
      "       0.94117647, 0.94509804, 0.94901961, 0.95294118, 0.95686275,\n",
      "       0.96078431, 0.96470588, 0.96862745, 0.97254902, 0.97647059,\n",
      "       0.98039216, 0.98431373, 0.98823529, 0.99215686, 0.99607843,\n",
      "       1.        ]), array([    3,  6821,  4158,  2321,  2200,  2085,  2027,  1988,  1914,\n",
      "        1801,  1792,  1766,  1696,  1693,  1694,  1694,  1755,  1781,\n",
      "        1913,  2118,  2231,   570,   598,   613,   640,   689,   227,\n",
      "         214,   248,   219,   212,   209,   190,   200,   206,   174,\n",
      "         181,   184,   195,   202,   159,   210,   150,   182,   191,\n",
      "         186,   189,   179,   175,   175,   148,   155,   166,   184,\n",
      "         193,   198,   202,   203,   265,   206,   208,   264,   283,\n",
      "         303,   304,   264,   201,   177,   189,   164,   152,   145,\n",
      "         134,   136,   133,   138,   125,   125,   126,   129,   143,\n",
      "         157,   123,   113,   129,   125,   174,   149,   110,   103,\n",
      "         102,   124,   112,   125,   124,   121,   113,   108,   107,\n",
      "         106,   116,   114,   113,   113,   117,   107,   111,    93,\n",
      "          69,    87,   111,   102,   116,    85,    92,   101,    90,\n",
      "          96,   111,   113,   101,   133,   121,   101,   104,   108,\n",
      "         116,    98,    96,   220,    98,   101,   105,    96,    98,\n",
      "         110,   119,   113,   123,   101,    97,   100,   102,   109,\n",
      "         102,    97,   114,    92,   104,    98,    88,   131,   110,\n",
      "          81,   104,    94,    94,   108,    89,    73,    84,    86,\n",
      "          85,    68,    92,    89,   183,   100,    92,   102,   109,\n",
      "          88,    94,   100,    93,    80,    82,    90,    96,    88,\n",
      "          88,    93,    90,    92,    69,    95,    89,    74,    61,\n",
      "          75,    75,    88,    89,    84,    81,    81,   112,    71,\n",
      "          76,    82,   140,    87,    66,   120,    83,    68,    75,\n",
      "          65,    92,    71,    49,    63,    77,    74,    71,    84,\n",
      "          80,    80,    76,    87,    64,    59,    60,   102,   116,\n",
      "         111,    93,   142,   113,   148,   119,   185,   110,   101,\n",
      "         125,   136,   123,   140,   130,   153,   139,   134,   137,\n",
      "         136,   153,   258,   162,   189,   174,   230,   277,   348,\n",
      "         207,   442,  1699, 35373], dtype=int64))\n",
      "\n",
      "dst_host_same_srv_rate\n",
      "(array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
      "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
      "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
      "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
      "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
      "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
      "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
      "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
      "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
      "       0.99, 1.  ]), array([ 5879,  6351,  6058,  3741,  4945,  4689,  3274,  5485,  2690,\n",
      "        1614,   775,   453,   359,   360,   301,   348,   286,   283,\n",
      "         317,   232,   301,   237,   396,   334,   545,   738,   341,\n",
      "         310,   192,   238,   159,   192,   157,   217,   155,   189,\n",
      "         192,   137,   196,   168,   214,   148,   166,   152,   194,\n",
      "         196,   162,   205,   161,   205,   243,   172,   168,   183,\n",
      "         176,   188,   195,   203,   174,   161,   199,   199,   206,\n",
      "         158,   187,   245,   150,   231,   176,   213,   184,   220,\n",
      "         160,   203,   191,   240,   214,   166,   205,   193,   288,\n",
      "         184,   225,   255,   221,   258,   189,   205,   290,   333,\n",
      "         245,   326,   288,   355,   317,   411,   538,   386,   706,\n",
      "         655, 47059], dtype=int64))\n",
      "\n",
      "dst_host_diff_srv_rate\n",
      "(array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
      "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
      "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
      "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
      "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
      "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
      "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
      "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
      "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
      "       0.99, 1.  ]), array([45521,  6867,  5660,  2752,  2517,  6956,  9401, 16291,  6817,\n",
      "        2411,   440,   329,   298,   193,   197,   214,   151,   188,\n",
      "         207,   106,   188,   103,   171,   103,    81,   153,    63,\n",
      "          91,    44,   104,    50,    70,    49,   126,    34,    50,\n",
      "          59,    37,    71,    43,   104,    32,    42,    56,    49,\n",
      "          55,    49,    62,    47,    61,   233,   169,   158,    56,\n",
      "          34,    43,    52,    33,    56,    39,    63,    40,    68,\n",
      "          57,   120,    58,    42,    92,   121,    60,    46,    57,\n",
      "          62,   257,   227,    59,    53,    35,    34,    26,    38,\n",
      "          30,    72,    24,    33,    30,    22,    32,    29,    39,\n",
      "          35,    81,    37,    49,    43,    81,    57,    81,    33,\n",
      "          28,  2087], dtype=int64))\n",
      "\n",
      "dst_host_same_src_port_rate\n",
      "(array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
      "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
      "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
      "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
      "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
      "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
      "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
      "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
      "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
      "       0.99, 1.  ]), array([60436, 15616,  5269,  3001,  1894,  1464,  1156,   909,   964,\n",
      "         606,   586,   653,   653,   167,   676,   191,   160,   724,\n",
      "         161,   137,   749,   143,   170,   123,   158,   835,   116,\n",
      "         155,   131,   181,   135,   165,   154,   920,   129,   178,\n",
      "         155,   112,   163,   119,   165,   113,   124,    94,   148,\n",
      "         126,   110,   120,    80,   114,   998,    96,    82,    91,\n",
      "          74,    92,    87,    78,    84,    65,    83,    71,    86,\n",
      "          56,    66,    70,    44,   114,    60,    71,    64,    84,\n",
      "          49,    67,    53,    82,    72,    58,    69,    56,    96,\n",
      "          51,    73,   110,   101,    97,    74,    67,    74,    74,\n",
      "          54,    75,    53,    56,    55,    66,    88,    48,    31,\n",
      "          18,  9083], dtype=int64))\n",
      "\n",
      "attack\n",
      "(array([0, 1], dtype=object), array([56544, 58630], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for col in df_pre.columns:\n",
    "    print(\"\\n\"+col)\n",
    "    print(np.unique(df_pre[col], return_counts=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
